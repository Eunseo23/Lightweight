import re
import json
import time
import os
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
from transformers import AutoModel, AutoTokenizer
from sklearn.metrics.pairwise import cosine_similarity

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
model_name = "microsoft/codebert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
# ë””ë°”ì´ìŠ¤ ì •ë³´ ì¶œë ¥
if device == "cuda":
    print(f"âœ… Using GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}")
else:
    print("âš ï¸ Using CPU")

# ì—°ì‚°ì ë° ê¸°í˜¸ ëª©ë¡
operators_v1 = [r'==', r'!=', r'>=', r'<=', r'\+\+', r'--', r'\+=', r'-=', r'\*=', r'/=', r'%=', r'&=', r'\|=', r'\^=', r'>>=', r'<<=',
    r'=', r'<', r'>', r'\+', r'-', r'\*', r'/', r'%', r'&', r'\|', r'\^', r'~', r'!', r'>>', r'<<', r'\?', r':', r'\.']
code_delimiters = [r'\{', r'\}', r';', r'\(', r'\)', r'\.', r',']

def protect_bug_tags(text):
    text = re.sub(r'\s*<bug>\s*', '##BUGSTART##', text)
    text = re.sub(r'\s*</bug>\s*', '##BUGEND##', text)
    return text

def restore_bug_tags(text):
    text = text.replace('##BUGSTART##', '<bug> ').replace('##BUGEND##', ' </bug>')
    text = re.sub(r'<bug>\s*<bug>', '<bug>', text)
    text = re.sub(r'</bug>\s*</bug>', '</bug>', text)
    text = re.sub(r'<bug>\s+', '<bug> ', text)
    text = re.sub(r'\s+</bug>', ' </bug>', text)
    return text

def add_spaces_around_symbols(text, operators_v1, delimiters):
    for operator in operators_v1:
        text = re.sub(f'(?<=\\w)({operator})', r' \1 ', text)
        text = re.sub(f'({operator})(?=\\w)', r' \1 ', text)
    for delimiter in delimiters:
        text = re.sub(f'(?<=\\w)({delimiter})', r' \1 ', text)
        text = re.sub(f'({delimiter})(?=\\w)', r' \1 ', text)
        text = re.sub(f' *({delimiter}) *', r' \1 ', text)
    return text

def remove_spaces_in_double_char_operators(text):
    text = re.sub(r'\= \=', '==', text)
    text = re.sub(r'\+ \+', '++', text)
    text = re.sub(r'\- \-', '--', text)
    text = re.sub(r'\! \=', '!=', text)
    text = re.sub(r'\> \=', '>=', text)
    text = re.sub(r'\< \=', '<=', text)
    return text

def remove_extra_spaces(text):
    return re.sub(r'\s+', ' ', text)

def process_line(line):
    line = protect_bug_tags(line)
    line = add_spaces_around_symbols(line, operators_v1, code_delimiters)
    line = remove_spaces_in_double_char_operators(line)
    line = remove_extra_spaces(line)
    line = restore_bug_tags(line)
    return line

def get_embedding(text, model, tokenizer, max_length=512, overlap=100):
    inputs = tokenizer(text, return_tensors="pt", padding=False, truncation=False).to(device)
    input_ids = inputs["input_ids"].squeeze(0)
    seq_length = input_ids.shape[0]

    if seq_length <= max_length:
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=max_length).to(device)
        with torch.no_grad():
            outputs = model(**inputs)
        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()
    else:
        embeddings = []
        start = 0
        while start < seq_length:
            end = min(start + max_length, seq_length)
            chunk_text = tokenizer.decode(input_ids[start:end], skip_special_tokens=True)
            inputs_chunk = tokenizer(chunk_text, return_tensors="pt", padding=True, truncation=True, max_length=max_length).to(device)
            with torch.no_grad():
                outputs_chunk = model(**inputs_chunk)
            embeddings.append(outputs_chunk.last_hidden_state[:, 0, :].squeeze().cpu().numpy())
            start += max_length - overlap
        return np.mean(embeddings, axis=0)

def file_to_lines(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = [process_line(line.strip()) for line in file.readlines()]
        # lines = [process_line(line) for line in file.readlines()]
    return lines

def list_to_dataframe(lines):
    return pd.DataFrame({'java_code': lines})

def calculate_total_tokens(df, column_name):
    return sum(len(tokenizer.tokenize(str(text))) for text in df[column_name].astype(str))

def lightweight_context(dfc, indices, model, tokenizer, a, target_token_limit, max_length=512, overlap=100):
    """ì²­í¬ ë‹¨ìœ„ë¥¼ í¬í•¨í•˜ì—¬ ê¸´ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ë¬¸ë§¥ì„ ìš”ì•½"""
    # ê° bug_lineì— ëŒ€í•´ ê±°ë¦¬ ì ìˆ˜ ê³„ì‚°
    dis_scores = {f'dis_score_{index}': [calculate_dis_score(line_number, index) for line_number in dfc.index] for index in indices}
    dis_scores_df = pd.DataFrame(dis_scores)

    # ê° ë¼ì¸ë³„ ì„ë² ë”© ìˆ˜í–‰ (ì²­í¬ í¬í•¨)
    dfc['embedding'] = dfc['java_code'].apply(lambda x: get_embedding(x, model, tokenizer, max_length, overlap))

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    sim_scores = {}
    for target_index in indices:
        if target_index < len(dfc):
            target_embedding = [dfc.loc[target_index, 'embedding']]  # ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ê¸°
            similarities = [cosine_similarity(target_embedding, [emb])[0][0] for emb in dfc['embedding']]
            sim_scores[f'sim_scores_{target_index}'] = similarities

    # DataFrame ë³€í™˜
    sim_scores_df = pd.DataFrame(sim_scores)
    df_combined = pd.concat([dfc, sim_scores_df, dis_scores_df], axis=1)

    # ì´ ì ìˆ˜ ê³„ì‚°
    sum_scores = {}
    dis_keys = list(sorted(dis_scores.keys()))
    sim_keys = list(sorted(sim_scores.keys()))
    a = 0.5
    for dis_key, sim_key in zip(dis_keys, sim_keys):
        if dis_key in dis_scores and sim_key in sim_scores:  # í‚¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            sum_scores[dis_key] = [a * float(x) + (1 - a) * float(y) for x, y in zip(dis_scores[dis_key], sim_scores[sim_key])]

    list_length = len(next(iter(sum_scores.values()))) if sum_scores else 0
    total_scores1 = [0] * list_length
    for scores in sum_scores.values():
        total_scores1 = [total + score for total, score in zip(total_scores1, scores)]

    num_keys = len(sum_scores)
    total_scores = [score / num_keys for score in total_scores1]
    df_combined['total_scores'] = total_scores

    # **ë°˜ë³µí•´ì„œ ì œê±°í•˜ë©´ì„œ í† í° ê°œìˆ˜ë¥¼ 200 ì´í•˜ë¡œ ì¤„ì´ëŠ” ê³¼ì •**
    max_iterations = 1000  # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì„¤ì •
    iteration_count = 0  # í˜„ì¬ ë°˜ë³µ íšŸìˆ˜
    time_limit = 30  # 30ì´ˆ ì œí•œ
    start_time = time.time()

    while True:
        iteration_count += 1
        current_time = time.time()
        if current_time - start_time > time_limit:
            print(f"Time limit reached ({time_limit} seconds), breaking the loop.")
            break
        if iteration_count > max_iterations:
            print(f"Max iterations reached ({max_iterations}), breaking the loop.")
            break

        # ê°€ì¥ ë‚®ì€ ì ìˆ˜ì˜ í–‰ ì œê±°
        min_total_score_index = df_combined['total_scores'].idxmin()
        df_combined = df_combined.drop(min_total_score_index)

        # í˜„ì¬ java_codeë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
        lwm = ' '.join(df_combined['java_code'])

        # í† í° ê°œìˆ˜ ê³„ì‚°
        tokens = tokenizer.tokenize(str(lwm))

        if len(tokens) < target_token_limit:  # ëª©í‘œ í† í° ê°œìˆ˜ ì´í•˜ë¼ë©´ ì¤‘ì§€
            break

    # ìµœì¢…ì ìœ¼ë¡œ ê²°í•©ëœ `java_code` ë°˜í™˜
    lwct = "<context> {} </context>".format(' '.join(df_combined['java_code']).replace('\n', ''))

    return lwct


def find_top_n_matches(original_methods, methods_list, n):
    split_methods = []
    current_method = []
    for line in methods_list:
        if line.strip():
            current_method.append(line.strip())
        else:
            if current_method:
                split_methods.append(current_method)
                current_method = []
    if current_method:
        split_methods.append(current_method)

    original_methods_list = [line.strip() for line in original_methods if line.strip()]
    original_text = " ".join(original_methods_list)

    original_embedding = get_embedding(original_text, model, tokenizer)

    method_embeddings = [
        (idx, method, get_embedding(" ".join(method), model, tokenizer))
        for idx, method in enumerate(split_methods, start=1)
    ]

    similarities = []
    for method_idx, method_text, method_embedding in method_embeddings:
        similarity = cosine_similarity([original_embedding], [method_embedding])[0][0]
        similarities.append((method_idx, method_text, similarity))

    sorted_matches = sorted(similarities, key=lambda x: x[2], reverse=True)
    return [match[1] for match in sorted_matches[1:n+1]]

#ë°ì´í„°í”„ë ˆì„ë‚´ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ë¼ì¸ ì°¾ê¸°
def find_most_similar_line(bug_embeddings, dfc):
    if dfc["embedding"].isna().all():  # ëª¨ë“  embeddingì´ Noneì´ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return []

    # ìœ ì‚¬í•œ ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•  ì§‘í•© (ì¤‘ë³µ ë°©ì§€)
    similar_indices = set()

    # ëª¨ë“  bug_embeddingì— ëŒ€í•´ ìœ ì‚¬í•œ ì¸ë±ìŠ¤ ì°¾ê¸°
    embeddings = np.stack(dfc["embedding"].dropna().values)  # None ê°’ ì œê±° í›„ ë°°ì—´ ë³€í™˜

    for bug_embedding in bug_embeddings:
        similarities = cosine_similarity([bug_embedding], embeddings)[0]  # 1D ë°°ì—´

        best_idx = np.argmax(similarities)  # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ ì¸ë±ìŠ¤
        best_index = int(dfc.dropna(subset=["embedding"]).iloc[best_idx].name)  # ì›ë˜ ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        similar_indices.add(best_index)  # ì¤‘ë³µë˜ì§€ ì•Šê²Œ ì¶”ê°€

    return list(similar_indices)  # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜

def calculate_total_tokens(df, column_name):
    total_tokens = 0
    for text in df[column_name].astype(str):  # í•´ë‹¹ ì»¬ëŸ¼ì˜ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        tokens = tokenizer.tokenize(text)  # í…ìŠ¤íŠ¸ë¥¼ í† í°í™”
        total_tokens += len(tokens)  # í† í° ê°œìˆ˜ ì¶”ê°€
    return total_tokens

def count_tokens_at_indexes(df, different_indexes):
    total_tokens = 0

    # different_indexesì— í•´ë‹¹í•˜ëŠ” í–‰ë§Œ ì„ íƒí•˜ì—¬ í† í° ê°œìˆ˜ ê³„ì‚°
    for idx in different_indexes:
        java_code_text = df.loc[idx, 'java_code']  # í•´ë‹¹ ì¸ë±ìŠ¤ì˜ java_code í…ìŠ¤íŠ¸
        tokens = tokenizer.tokenize(str(java_code_text))  # í…ìŠ¤íŠ¸ë¥¼ í† í°í™”
        total_tokens += len(tokens)  # í† í° ê°œìˆ˜ ì¶”ê°€

    return total_tokens

# ìë°” ê¸°ë³¸ ë°ì´í„° íƒ€ì… ëª©ë¡
java_types = {
    'int', 'short', 'long', 'float', 'double', 'char', 'boolean', 'byte', 'String',
    'void', 'Integer', 'Boolean', 'Double', 'Character', 'Float', 'Long', 'Short', 'Byte'}

# ì—°ì‚°ì ëª©ë¡
operators = {'=', '+', '-', '*', '/', '%', '==', '!=', '>', '<', '>=', '<=', '&&', '||', '!', '&', '|', '^', '<<', '>>', '>>>', '+=', '-=', '*=', '/=', '%=', '&=', '|=', '^='}

# indexë³„ë¡œ ê±°ë¦¬ ì ìˆ˜ êµ¬í•˜ê¸°
def calculate_dis_score(line_number, bug_line):
    if line_number == bug_line:
        return 1.0
    result = 1.0 / (abs(bug_line - line_number) + 1)
    return result

def lightweightdefects4j(buggy_method_lines, context_lines, a=0.5):
    df = list_to_dataframe(buggy_method_lines)
    dfc = list_to_dataframe(context_lines)
    bug_indices = df[df['java_code'].fillna('').str.strip().str.startswith('<bug>')].index.tolist()

    total_tokens_df = calculate_total_tokens(df, 'java_code')
    total_tokens_dfc = calculate_total_tokens(dfc, 'java_code')

    if total_tokens_df < 700:
        # lwbmo = ' '.join(df['java_code'])
        lwbmo = ' '.join(line.strip() for line in df['java_code'])
        bug_contents = re.findall(r"<bug>(.*?)</bug>", lwbmo) if re.search(r"<bug>(.*?)</bug>", lwbmo) else []
        bug_embeddings = [get_embedding(content.strip(), model, tokenizer) for content in bug_contents] if bug_contents else []

        if total_tokens_dfc < 300 + (700 - total_tokens_df):
            lwct = "<context> {} </context>".format(' '.join(dfc['java_code']).replace('\n', ' '))
        else:
            dfc["embedding"] = dfc["java_code"].apply(lambda x: get_embedding(x, model, tokenizer) if x.strip() else None)
            dfc_indices = find_most_similar_line(bug_embeddings, dfc)
            lwct = lightweight_context(dfc, dfc_indices, model, tokenizer, a, target_token_limit=300 + (700 - total_tokens_df))

        return f'{lwbmo} {lwct}'.strip()
    else:
        dis_scores = {
            f'dis_score_{bug_line}': [calculate_dis_score(line_number, bug_line) for line_number in df.index]
            for bug_line in bug_indices
        }
        dis_scores_df = pd.DataFrame(dis_scores)
        df['embedding'] = df['java_code'].apply(lambda x: get_embedding(x, model, tokenizer))

        sim_scores = {}
        for target_index in bug_indices:
            if target_index < len(df):
                target_embedding = [df.loc[target_index, 'embedding']]
                similarities = [cosine_similarity(target_embedding, [emb])[0][0] for emb in df['embedding']]
                sim_scores[f'sim_scores_{target_index}'] = similarities

        sim_scores_df = pd.DataFrame(sim_scores)
        df_combined = pd.concat([df, sim_scores_df, dis_scores_df], axis=1)

        sum_scores = {}
        # a = 0.3
        for dis_key, sim_key in zip(sorted(dis_scores.keys()), sorted(sim_scores.keys())):
            if dis_key in dis_scores and sim_key in sim_scores:
                sum_scores[dis_key] = [a * float(x) + (1 - a) * float(y) for x, y in zip(dis_scores[dis_key], sim_scores[sim_key])]

        total_scores = np.mean(list(sum_scores.values()), axis=0) if sum_scores else []
        df_combined['total_scores'] = total_scores
        # df_combined.drop(columns=['embedding'], errors='ignore').to_csv("df_with_scores.csv", index=False) ##################################

        max_iterations = 1000
        time_limit = 30
        start_time = time.time()
        iteration_count = 0

        while True:
            iteration_count += 1
            if time.time() - start_time > time_limit or iteration_count > max_iterations:
                break

            min_total_score_index = df_combined['total_scores'].idxmin()
            df_combined = df_combined.drop(min_total_score_index)

            lwbm = ''.join(df_combined['java_code'])
            tokens = tokenizer.tokenize(str(lwbm))
            if len(tokens) < 700:
                break

        # lwbmo = ' '.join(df_combined['java_code'])
        lwbmo = ' '.join(line.strip() for line in df_combined['java_code'])
        bug_contents = re.findall(r"<bug>(.*?)</bug>", lwbmo) if re.search(r"<bug>(.*?)</bug>", lwbmo) else []
        bug_embeddings = [get_embedding(content.strip(), model, tokenizer) for content in bug_contents] if bug_contents else []

        if total_tokens_dfc < 300:
            lwct = "<context> {} </context>".format(' '.join(dfc['java_code']).replace('\n', ' '))
        else:
            dfc["embedding"] = dfc["java_code"].apply(lambda x: get_embedding(x, model, tokenizer) if x.strip() else None)
            dfc_indices = find_most_similar_line(bug_embeddings, dfc)
            lwct = lightweight_context(dfc, dfc_indices, model, tokenizer, a, target_token_limit=300)

        return f'{lwbmo} {lwct}'.strip()

###################################################################################
def process_inner_folder(inner_path, case_path):
    """CategoryPlot ê°™ì€ ì‹¤ì œ íŒŒì¼ì´ ìˆëŠ” í´ë” ì²˜ë¦¬"""

    # methods íŒŒì¼ ì°¾ê¸°
    method_files = [f for f in os.listdir(inner_path) if f.endswith("_methods.txt")]
    if not method_files:
        print(f"âš ï¸ {inner_path} - methods íŒŒì¼ ì—†ìŒ. ê±´ë„ˆëœ€.")
        return
    methods_path = os.path.join(inner_path, method_files[0])
    methods_lines = file_to_lines(methods_path)

    # buggy íŒŒì¼ ì°¾ê¸°
    buggy_files = [f for f in os.listdir(inner_path) if "Original_buggy_method_by_line" in f and f.endswith(".txt")]
    if not buggy_files:
        print(f"âš ï¸ {inner_path} - buggy íŒŒì¼ ì—†ìŒ. ê±´ë„ˆëœ€.")
        return

    folder_name = os.path.basename(inner_path)
    a_value_map = {
        "50%": 0.5,
        "0%": 0.0,
        "100%": 1.0
    }

    for buggy_file in sorted(buggy_files):
        buggy_path = os.path.join(inner_path, buggy_file)
        buggy_lines = file_to_lines(buggy_path)

        # top 5 context
        top_contexts = find_top_n_matches(buggy_lines, methods_lines, 5)

        json_results = []
        for label, a in a_value_map.items():
            for i, context in enumerate(top_contexts, start=1):
                context_lines = context
                lwbm_result = lightweightdefects4j(buggy_lines, context_lines, a=a)
                json_results.append({
                    "id": f"{i}_{label}",
                    "lwbm": lwbm_result
                })

        final_output = {folder_name: json_results}

        # íŒŒì¼ëª… ë³€í™˜
        # ì˜ˆ: CategoryPlot_Original_buggy_method_by_line1.txt â†’
        #     CategoryPlot_buggy_and_context_method1.json
        if "Original_buggy_method_by_line" in buggy_file:
            prefix, _, suffix = buggy_file.partition("Original_buggy_method_by_line")
            number_part = ''.join(ch for ch in suffix if ch.isdigit())
            save_name = f"{prefix}Lightweight_method_llama_diff{number_part}.json"
        else:
            save_name = "Lightweight_method_llama_diff.json"

        # ì €ì¥: inner_path (CategoryPlot) ì™€ case_path ë‘ ê³³ì— ì €ì¥
        save_path_inner = os.path.join(inner_path, save_name)
        save_path_outer = os.path.join(case_path, save_name)

        for save_path in [save_path_inner, save_path_outer]:
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(final_output, f, indent=2, ensure_ascii=False)
            print(f"âœ… {save_path} ì €ì¥ ì™„ë£Œ")


def process_case_folder(case_path):
    # case_folder ë°”ë¡œ ì•ˆì—ì„œ ë¨¼ì € ì‹œë„
    process_inner_folder(case_path, case_path)

    # ê·¸ë¦¬ê³  ì„œë¸Œí´ë”ë“¤ ìˆœíšŒ
    subfolders = [os.path.join(case_path, d) for d in os.listdir(case_path) if os.path.isdir(os.path.join(case_path, d))]
    for inner in subfolders:
        process_inner_folder(inner, case_path)
# #############################################################################
if __name__ == "__main__":

    base_dir = "to/your/path"
    project_folder = "multi"
    project_path = os.path.join(base_dir, project_folder)

    if not os.path.isdir(project_path):
        print(f"ğŸš« {project_folder} í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ.")
    else:
        for case_folder in sorted(os.listdir(project_path)):
            case_path = os.path.join(project_path, case_folder)
            if not os.path.isdir(case_path):
                continue
            process_case_folder(case_path)    
    
##í•˜ë‚˜ì˜ í´ë”ì—ì„œ í•˜ë‚˜ë§Œ ì²˜ë¦¬
# if __name__ == "__main__":

#     methods_path = "to/your/path"
#     buggy_path = "to/your/path"

#     methods_lines = file_to_lines(methods_path)
#     buggy_lines = file_to_lines(buggy_path)
#     top_contexts = find_top_n_matches(buggy_lines, methods_lines, 5)
    
#     folder_name = "Cli-12"
#     a_value_map = {
#         "50%": 0.5,
#         # "0%": 0.0,
#         # "100%": 1.0
#     }
#     json_results = []
#     for label, a in a_value_map.items():
#         for i, context in enumerate(top_contexts, start=1):
#             context_lines = context
#             lwbm_result = lightweightdefects4j(buggy_lines, context_lines, a=a)
#             json_results.append({
#                 "id": f"{i}_{label}",   # <-- idë¥¼ 1_30%, 2_30%, 1_0%, 2_0% ì´ëŸ° ì‹ìœ¼ë¡œ
#                 "lwbm": lwbm_result
#             })
#     final_output = {
#         folder_name: json_results
#     }
#     save_dir = "to/your/path"  # ì›í•˜ëŠ” ì €ì¥ í´ë”
#     os.makedirs(save_dir, exist_ok=True)  # í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
#     save_path = os.path.join(save_dir, "Lightweight_buggy_method_Context_llama.json")

#     with open(save_path, "w", encoding="utf-8") as f:
#         json.dump(final_output, f, indent=2, ensure_ascii=False)

#     print("âœ… Saved Lightweight_buggy_method_Context_llama.json")







# ====== ë©”ì¸ ì‹¤í–‰ ì˜ì—­ ======
# if __name__ == "__main__":

#     base_dir = "to/your/path"

#     project_folder = "Codellama"  # íŠ¹ì • í´ë”ë§Œ ì²˜ë¦¬
#     project_path = os.path.join(base_dir, project_folder)

#     if not os.path.isdir(project_path):
#         print(f"ğŸš« {project_folder} í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ.")
#     else:
#         for case_folder in sorted(os.listdir(project_path)):
#             case_path = os.path.join(project_path, case_folder)
#             if not os.path.isdir(case_path):
#                 continue

#             buggy_file = os.path.join(case_path, "Original_buggy_method_by_line.txt")
#             method_files = [f for f in os.listdir(case_path) if f.endswith("_methods.txt")]

#             if not os.path.isfile(buggy_file) or not method_files:
#                 print(f"âš ï¸ {case_path} - í•„ìš”í•œ íŒŒì¼ ì—†ìŒ. ê±´ë„ˆëœ€.")
#                 continue

#             methods_path = os.path.join(case_path, method_files[0])
#             methods_lines = file_to_lines(methods_path)
            
#             buggy_lines = file_to_lines(buggy_file)
#             # print(buggy_lines)
#             top_contexts = find_top_n_matches(buggy_lines, methods_lines, 5)

#             folder_name = os.path.basename(case_path)

#             a_value_map = {
#                 "50%": 0.5,
#                 "0%": 0.0,
#                 "100%": 1.0
#             }

#             json_results = []
#             for label, a in a_value_map.items():
#                 for i, context in enumerate(top_contexts, start=1):
#                     context_lines = context
#                     lwbm_result = lightweightdefects4j(buggy_lines, context_lines, a=a)
#                     json_results.append({
#                         "id": f"{i}_{label}",
#                         "lwbm": lwbm_result
#                     })

#             final_output = {
#                 folder_name: json_results
#             }

#             save_path = os.path.join(case_path, "Lightweight_method_llama_diff.json")
#             with open(save_path, "w", encoding="utf-8") as f:
#                 json.dump(final_output, f, indent=2, ensure_ascii=False)

#             print(f"âœ… {case_path} - Lightweight_method_llama_diff.json ì €ì¥ ì™„ë£Œ")
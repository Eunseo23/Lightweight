import os
import fnmatch 
import json
import torch
import torch.nn as nn
from tqdm import tqdm
from transformers import RobertaTokenizer, RobertaConfig, RobertaModel
from itertools import chain
from model import Seq2Seq  # ë„ˆì˜ model.pyì— ì •ì˜ëœ í´ë˜ìŠ¤

# ====== ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸° ======
def load_model_and_tokenizer():
    tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")
    tokenizer.add_special_tokens({"additional_special_tokens": ["<bug>", "</bug>", "<context>", "</context>"]})
    # tokenizer.add_tokens(["<bug>", "</bug>", "<context>", "</context>", "<fix>", "</fix>"])

    config = RobertaConfig.from_pretrained("microsoft/codebert-base")
    encoder = RobertaModel.from_pretrained("microsoft/codebert-base", config=config)
    encoder.resize_token_embeddings(len(tokenizer))

    decoder_layer = nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads)
    decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)

    model = Seq2Seq(
        encoder=encoder,
        decoder=decoder,
        config=config,
        beam_size=100,  # âœ… beam size 100ìœ¼ë¡œ ì„¤ì •
        max_length=512,
        sos_id=tokenizer.cls_token_id,
        eos_id=tokenizer.sep_token_id,
    )

    model.load_state_dict(torch.load(model_path, map_location="cuda" if torch.cuda.is_available() else "cpu"))
    model.to("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()
    return model, tokenizer

# ====== ì˜ˆì¸¡ í•¨ìˆ˜ ======
def generate_single_prediction(text, model, tokenizer, device):
    tokens = tokenizer.tokenize(text)[:510]
    tokens = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]
    source_ids = tokenizer.convert_tokens_to_ids(tokens)
    source_mask = [1] * len(source_ids)

    padding_length = 512 - len(source_ids)
    source_ids += [tokenizer.pad_token_id] * padding_length
    source_mask += [0] * padding_length

    source_ids = torch.tensor([source_ids], dtype=torch.long).to(device)
    source_mask = torch.tensor([source_mask], dtype=torch.long).to(device)

    with torch.no_grad():
        outputs = model(source_ids=source_ids, source_mask=source_mask)

    # Beam ê²°ê³¼ê°€ í•˜ë‚˜ì˜ í…ì„œë¡œ ë¬¶ì—¬ ìˆìœ¼ë©´ outputs: [beam_size, max_length]
    # ë¦¬ìŠ¤íŠ¸ ì•ˆì— í…ì„œë“¤ì´ ìˆëŠ” êµ¬ì¡°ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if isinstance(outputs, list):
        outputs = torch.stack(outputs, dim=0)  # ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜

    predictions = []
    for i in range(outputs.size(0)):
        tokens = outputs[i].cpu().numpy().tolist()
        if isinstance(tokens[0], list):  # âœ… ë¦¬ìŠ¤íŠ¸ ì•ˆ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
            tokens = list(chain.from_iterable(tokens))
        if 0 in tokens:
            tokens = tokens[:tokens.index(0)]
        decoded = tokenizer.decode(tokens, clean_up_tokenization_spaces=True).strip()
        predictions.append(decoded)

    return predictions[:]  # âœ… beam í›„ë³´ ì „ì²´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

# ====== JSON ì²˜ë¦¬ ë° ì˜ˆì¸¡ ì €ì¥ ======
def process_json_file(json_path, model, tokenizer):
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        folder_key = next(iter(data.keys()))
        items = data[folder_key]
        global_id = 1

        for item in tqdm(items, desc=f"Generating lwcp for {folder_key}"):
            lwbm_input = item["lwbm"]

            pred_list = generate_single_prediction(lwbm_input, model, tokenizer, device)
            unique_predictions = sorted(set(pred_list))[:100]  # ì¤‘ë³µ ì œê±°
            item["lwcp"] = {f"lwcp{global_id + i}": p for i, p in enumerate(unique_predictions)}
            global_id += 100

        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        print(f"âœ… {json_path} ì²˜ë¦¬ ì™„ë£Œ. ëª¨ë“  í•­ëª©ì— lwcp1~100 ì €ì¥ë¨.")
        return True

    except Exception as e:
        print(f"âŒ {json_path} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

# # ====== ì „ì²´ í´ë” ë‚´ JSON ìˆœíšŒ ì²˜ë¦¬ ======
# def process_all_json_in_base_dir(base_dir, model, tokenizer):
#     failed_folders = []

#     for mid_folder in sorted(os.listdir(base_dir)):
#         mid_folder_path = os.path.join(base_dir, mid_folder)
        model_emb = encoder.get_input_embeddings()
        with torch.no_grad():
            # Xavier Uniform initialization ensures better weight distribution
            model_emb.weight[-num_added_tokens:] = torch.nn.init.xavier_uniform_(torch.empty(num_added_tokens, config.hidden_size))

#         if not os.path.isdir(mid_folder_path):
#             continue

#         for chart_folder in sorted(os.listdir(mid_folder_path)):
#             chart_folder_path = os.path.join(mid_folder_path, chart_folder)
#             if not os.path.isdir(chart_folder_path):
#                 continue

#             json_files = [f for f in os.listdir(chart_folder_path) if f.endswith(".json")]
#             if not json_files:
#                 print(f"âš ï¸ {chart_folder_path} ë‚´ JSON íŒŒì¼ ì—†ìŒ. ê±´ë„ˆëœ€.")
#                 continue

#             for json_file in json_files:
#                 json_path = os.path.join(chart_folder_path, json_file)
#                 success = process_json_file(json_path, model, tokenizer)
#                 if not success:
#                     failed_folders.append(chart_folder_path)

#     if failed_folders:
#         print("\nâ— ì²˜ë¦¬ ì‹¤íŒ¨í•œ í´ë” ëª©ë¡:")
#         for folder in failed_folders:
#             print("  -", folder)
#     else:
#         print("\nğŸ‰ ëª¨ë“  JSON íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")

# ====== ì „ì²´ í´ë” ë‚´ íŠ¹ì • JSONíŒŒì¼ë§Œ ìˆœíšŒ ì²˜ë¦¬ ======
def process_all_json_in_base_dir(base_dir, model, tokenizer):
    failed_folders = []
    target_folders = ["DiffBERT"]

    for project_folder in sorted(os.listdir(base_dir)):
        if project_folder not in target_folders:
        # if project_folder != "Lightweight":  # âœ… Lightweight í´ë”ë§Œ ì²˜ë¦¬
        # if project_folder == "Lightweight":  # âœ… Lightweight í´ë”ë§Œ ì œì™¸
            print(f"ğŸš« {project_folder} í´ë”ëŠ” ìŠ¤í‚µë¨.")
            continue

        project_path = os.path.join(base_dir, project_folder)
        if not os.path.isdir(project_path):
            continue

        for case_folder in sorted(os.listdir(project_path)):
            case_path = os.path.join(project_path, case_folder)
            if not os.path.isdir(case_path):
                continue

            # âœ… ëª¨ë“  ì ‘ë‘ì–´ë¥¼ í—ˆìš©í•˜ëŠ” JSON íŒ¨í„´
            json_candidates = [f for f in os.listdir(case_path) 
                               if fnmatch.fnmatch(f, "*Lightweight_method_BERT_diff*.json")]
                            #    if fnmatch.fnmatch(f, "*buggy_and_context_method*.json") and "diff" not in f]

            if not json_candidates:
                print(f"âš ï¸ {case_path} ë‚´ ëŒ€ìƒ JSON ì—†ìŒ. ê±´ë„ˆëœ€.")
                continue

            for json_filename in json_candidates:
                json_path = os.path.join(case_path, json_filename)
                success = process_json_file(json_path, model, tokenizer)
                if not success:
                    failed_folders.append(case_path)

    if failed_folders:
        print("\nâ— ì²˜ë¦¬ ì‹¤íŒ¨í•œ í´ë” ëª©ë¡:")
        for folder in failed_folders:
            print("  -", folder)
    else:
        print("\nğŸ‰ ëª¨ë“  JSON íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")


# ====== ì„¤ì • ======
model_path = "./Lightweight-main/output/checkpoint-last-0316-1124/pytorch_model_0316-1124.bin"
base_dir = "to/your/path"

if __name__ == "__main__":
    model, tokenizer = load_model_and_tokenizer()
    process_all_json_in_base_dir(base_dir, model, tokenizer)


# ====== ë‹¨ì¼ í´ë” ë‚´ JSON ìˆœíšŒ ì²˜ë¦¬ ======
# def process_all_json_in_target_folder(target_folder, model, tokenizer):
#     failed_files = []

#     if not os.path.isdir(target_folder):
#         print(f"â— ìœ íš¨í•˜ì§€ ì•Šì€ í´ë” ê²½ë¡œì…ë‹ˆë‹¤: {target_folder}")
#         return

#     json_files = [f for f in os.listdir(target_folder) if f.endswith(".json")]
#     if not json_files:
#         print(f"âš ï¸ {target_folder} ë‚´ JSON íŒŒì¼ ì—†ìŒ. ê±´ë„ˆëœ€.")
#         return

#     for json_file in json_files:
#         json_path = os.path.join(target_folder, json_file)
#         success = process_json_file(json_path, model, tokenizer)
#         if not success:
#             failed_files.append(json_file)

#     if failed_files:
#         print("\nâ— ì²˜ë¦¬ ì‹¤íŒ¨í•œ JSON íŒŒì¼ ëª©ë¡:")
#         for file in failed_files:
#             print("  -", file)
#     else:
#         print(f"\nğŸ‰ ëª¨ë“  JSON íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ! (ì´ {len(json_files)}ê°œ)")

# # ====== ì„¤ì • ======
# model_path = "./Lightweight-main/python/output/checkpoint-last-0316-1124/pytorch_model_0316-1124.bin"
# target_folder = "to/your/path"  # ğŸ” ì—¬ê¸°ë¥¼ ì›í•˜ëŠ” í´ë”ë¡œ ìˆ˜ì •

# if __name__ == "__main__":
#     model, tokenizer = load_model_and_tokenizer()
#     process_all_json_in_target_folder(target_folder, model, tokenizer)


# ====== ë‹¨ì¼ í´ë” ë‚´ íŠ¹ì •JSONíŒŒì¼ë§Œ ìˆœíšŒ ì²˜ë¦¬ ======
# def process_specific_json_in_target_folder(target_folder, model, tokenizer, target_keyword):
#     failed_files = []

#     if not os.path.isdir(target_folder):
#         print(f"â— ìœ íš¨í•˜ì§€ ì•Šì€ í´ë” ê²½ë¡œì…ë‹ˆë‹¤: {target_folder}")
#         return

#     # í´ë” ë‚´ì—ì„œ target_keywordë¥¼ í¬í•¨í•˜ê³  .json í™•ì¥ìì¸ ëª¨ë“  íŒŒì¼ì„ ì°¾ìŒ
#     json_files = [
#         f for f in os.listdir(target_folder)
#         if f.endswith(".json") and target_keyword in f
#     ]

#     if not json_files:
#         print(f"âš ï¸ {target_folder} ë‚´ '{target_keyword}' í¬í•¨ëœ JSON ì—†ìŒ. ê±´ë„ˆëœ€.")
#         return

#     for json_file in json_files:
#         json_path = os.path.join(target_folder, json_file)
#         success = process_json_file(json_path, model, tokenizer)
#         if not success:
#             failed_files.append(json_file)

#     if failed_files:
#         print("\nâ— ì²˜ë¦¬ ì‹¤íŒ¨í•œ JSON íŒŒì¼ ëª©ë¡:")
#         for file in failed_files:
#             print("  -", file)
#     else:
#         print(f"\nğŸ‰ '{target_keyword}' í¬í•¨ëœ ëª¨ë“  JSON ì²˜ë¦¬ ì™„ë£Œ!")

# def process_multiple_folders(base_path, folder_list, model, tokenizer, target_keyword):
#     for folder_name in folder_list:
#         target_folder = os.path.join(base_path, folder_name)
#         process_specific_json_in_target_folder(target_folder, model, tokenizer, target_keyword)

# # ====== ì„¤ì • ======
# model_path = "./Lightweight-main/python/output/checkpoint-last-0316-1124/pytorch_model_0316-1124.bin"
# base_path = "to/your/path"  # ì—¬ëŸ¬ í´ë”ë“¤ì´ ìˆëŠ” ê¸°ë³¸ ê²½ë¡œ
# # target_folders = ["Closure_65", "Chart_4", "Chart_26","Closure_66","Closure_119", "Chart_13", "Closure_71", "Closure_123","Lang_16","Lang_58", "Math_24"]  # ğŸ”¥ ì²˜ë¦¬í•˜ê³  ì‹¶ì€ í´ë”ë“¤ ë¦¬ìŠ¤íŠ¸
# target_folders = ["Math_38","Closure_65"] 
# target_keyword = "Lightweight_buggy_method_Context_diff"

# if __name__ == "__main__":
#     model, tokenizer = load_model_and_tokenizer()
#     # process_multiple_folders(base_path, target_folders, model, tokenizer, target_filename)
#     process_multiple_folders(base_path, target_folders, model, tokenizer, target_keyword)
#     # process_specific_json_in_target_folder(target_folders, model, tokenizer, target_filename)
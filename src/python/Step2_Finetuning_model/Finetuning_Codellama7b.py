import os
import torch
import re
from transformers import TrainingArguments, default_data_collator
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, PeftModel, TaskType
from transformers import Trainer
from datasets import Dataset
import pandas as pd
import math
import json
import time
from transformers import TrainerCallback
from tqdm.auto import tqdm
import inspect
from transformers import TrainingArguments

# 1. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
token = "your/token/info"  # Hugging Faceì—ì„œ ë°œê¸‰ë°›ì€ í† í°

model = AutoModelForCausalLM.from_pretrained(
    "codellama/CodeLlama-7b-hf",
    token=token,
    torch_dtype=torch.float16,           # âœ… fp16
    device_map="auto"                    # âœ… ìë™ GPU í• ë‹¹
)

tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-hf",use_fast=False, token=token)
tokenizer.pad_token = tokenizer.eos_token

# 2. LoRA ì„¤ì •
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    # ["q_proj", "v_proj"]
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)



# 4. íŠ¹ìˆ˜ í† í° ì •ì˜
# special_tokens = {"additional_special_tokens": ["<bug>", "</bug>", "<context>", "</context>", "<fix>", "</fix>"]}
special_tokens = {"additional_special_tokens": ["[bug_function], [reference_function], [fix_code]"]}

# 5. í† í¬ë‚˜ì´ì €ì— ì¶”ê°€ / ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ ì¡°ì •
tokenizer.add_special_tokens(special_tokens)
model.resize_token_embeddings(len(tokenizer))
embedding_weights = model.get_input_embeddings().weight
print("Embedding shape:", embedding_weights.shape)

# 3. LoRA ì ìš©
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# def is_deletion_case(buggy_code: str, fixed_code: str) -> bool:
#     """
#     fixed codeê°€ // fix_start // fix_end ì¸ ê²½ìš° ì°¾ê¸°
#     """
#     bug_block = extract_bug_block(buggy_code)
#     buggy_tokens = set(bug_block.strip().split())
#     fixed_tokens = set(fixed_code.strip().split())

#     deleted_tokens = buggy_tokens - fixed_tokens
#     inserted_or_modified = fixed_tokens - buggy_tokens

#     return len(deleted_tokens) > 0 and len(inserted_or_modified) == 0

def load_all_pf_pairs_from_dict(data, parent_key=""):
    pf_pairs = []

    if isinstance(data, list):
        top_level_items = enumerate(data)
    elif isinstance(data, dict):
        top_level_items = data.items()
    else:
        return pf_pairs

    for key, entry in top_level_items:
        if not isinstance(entry, dict):
            continue

        p_dict = {}
        f_dict = {}

        for file_key, value in entry.items():
            if file_key.endswith("_P"):
                base = file_key[:-2]
                p_dict[base] = value
            elif file_key.endswith("_F"):
                base = file_key[:-2]
                f_dict[base] = value

        for base in set(p_dict.keys()) & set(f_dict.keys()):
            pf_pairs.append({
                "source_file": str(parent_key),
                "group": str(key),
                "base": base,
                "buggy_code": p_dict[base],
                "fixed_code": f_dict[base]
            })

    return pf_pairs

def collect_all_pf_pairs_from_directory(directory_path):
    all_pairs = []

    for filename in os.listdir(directory_path):
        if filename.endswith(".json"):
            file_path = os.path.join(directory_path, filename)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                pairs = load_all_pf_pairs_from_dict(data, parent_key=filename)
                all_pairs.extend(pairs)
            except Exception as e:
                print(f"âŒ íŒŒì¼ '{filename}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return all_pairs

# ì •í™•í•œ íŒŒì¼ ê²½ë¡œ (MyDrive ê¸°ì¤€)
json_path = "/home/selab/Desktop/Dataset/Dataset_ver2"
pf_pairs = collect_all_pf_pairs_from_directory(json_path)

# 5. buggy/fixed ì½”ë“œ ì¶”ì¶œ ë° í•„í„°ë§
processed_data = []
before_count = 0
deletion_samples = []

# prefix_prompt = "Fix each <bug> block. Use <context> only for reference.\n"
    
for pair in pf_pairs:
    buggy_code = pair["buggy_code"]
    fixed_code = pair["fixed_code"]
    before_count += 1

    # ê³µí†µ ë°ì´í„°ëŠ” í•­ìƒ í¬í•¨
    sample = {
        "input": buggy_code,
        "target": fixed_code
    }
    processed_data.append(sample)
    
    # ë¹„ì–´ ìˆëŠ” <fix> </fix> ë§Œ ìˆëŠ” ê²½ìš° ìˆ˜ì§‘
    marker = "\n// fix_start\n \n// fix_end\n"
    if marker in fixed_code:
        deletion_samples.append(sample)
        
# â« oversampling: ì‚­ì œ ì¼€ì´ìŠ¤ 3ë°° ì¶”ê°€
processed_data.extend(deletion_samples * 7)  # ì´ 8ë°° (1 + 7)

# 2. í†µê³„ ì¶œë ¥
print("ì´ ìƒ˜í”Œ ìˆ˜:", len(processed_data))
print("ì‚­ì œ ì¼€ì´ìŠ¤ ìˆ˜:", len(deletion_samples))
print(f"ìµœì¢… í•™ìŠµ ìƒ˜í”Œ ìˆ˜ (oversampled): {len(processed_data)}")

# ì „ì²´ ì¹´ìš´íŠ¸ ì¶œë ¥
# print(f"\nì „ì²´ bug-fix ìŒ ìˆ˜ (ì›ë³¸): {before_count}")
# print(f"ì‚­ì œ ì „ìš© ìŒ ìˆ˜: {len(deletion_samples)}")
# print(f"ìµœì¢… í•™ìŠµ ìƒ˜í”Œ ìˆ˜ (oversampled): {len(processed_data)}")

# #ë°ì´í„° ìŒ ê°œìˆ˜ í™•ì¸
# print(f"Before filtering: {before_count} bug-fix pairs")
# print(f"After filtering: {after_count} bug-fix pairs")

# 6. Pandas DataFrameìœ¼ë¡œ ë³€í™˜
df = pd.DataFrame(processed_data)

# 7. Hugging Face Datasetìœ¼ë¡œ ë³€í™˜
dataset = Dataset.from_pandas(df)

# 8. ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_function(examples):
    inputs = examples["input"]
    targets = examples["target"]

    input_ids_list = []
    attention_mask_list = []
    labels_list = []

    for input_text, target_text in zip(inputs, targets):
        prefix = ("\n[bug_function]\n" + input_text + "\n[fix_code]\n")
        full_text = prefix +  target_text

        # ì „ì²´ ì‹œí€€ìŠ¤ í† í¬ë‚˜ì´ì¦ˆ
        tokenized_full = tokenizer(
            full_text,
            max_length=2048,
            truncation=True,
            padding="max_length",
            add_special_tokens=True
        )

        # input ê¸¸ì´ ê³„ì‚°
        tokenized_input = tokenizer(
            prefix,
            truncation=True,
            max_length=1024,
            padding=False,
            add_special_tokens=True
        )
        input_len = len(tokenized_input["input_ids"])

        # labels êµ¬ì„±
        labels = tokenized_full["input_ids"].copy()
        labels[:input_len] = [-100] * input_len
        labels = [l if l != tokenizer.pad_token_id else -100 for l in labels]

        # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        input_ids_list.append(tokenized_full["input_ids"])
        attention_mask_list.append(tokenized_full["attention_mask"])
        labels_list.append(labels)

    return {
        "input_ids": input_ids_list,
        "attention_mask": attention_mask_list,
        "labels": labels_list
    }

# 9. ì „ì²˜ë¦¬ + ë¶„í• 
split_dataset = dataset.train_test_split(test_size=0.2)
tokenized_dataset = split_dataset.map(preprocess_function, batched=True)
train_dataset = tokenized_dataset["train"]
eval_dataset = tokenized_dataset["test"]

# í•™ìŠµì„¤ì •
# print(TrainingArguments)
training_args = TrainingArguments(
    output_dir="./codellama-lora-output",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=2,
    warmup_steps=200,
    num_train_epochs=2,
    learning_rate=5e-5,
    lr_scheduler_type="linear",
    fp16=True,  # GPUê°€ AMP ì§€ì›í•  ê²½ìš°
    logging_steps=1000,
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=1,
    report_to="none",
    disable_tqdm=False,
    remove_unused_columns=False
)

# Data Collator êµ¬ì„±
data_collator = default_data_collator
    
class TqdmProgressCallback(TrainerCallback):
    def __init__(self):
        self.pbar = None

    def on_train_begin(self, args, state, control, **kwargs):
        self.pbar = tqdm(total=state.max_steps, desc="Training", position=0)

    def on_log(self, args, state, control, logs=None, **kwargs):
        if self.pbar and logs is not None:
            desc = f"Step {state.global_step} | "

            if "loss" in logs:
                desc += f"Loss: {logs['loss']:.4f} | "

            if "eval_loss" in logs:
                desc += f"Eval Loss: {logs['eval_loss']:.4f} | "
                # Perplexity ê³„ì‚°
                try:
                    ppl = math.exp(logs["eval_loss"])
                    desc += f"PPL: {ppl:.2f}"
                except OverflowError:
                    desc += "PPL: inf"

            if "epoch" in logs:
                desc += f" | Epoch: {logs['epoch']:.2f}"

            self.pbar.set_description_str(desc)

    def on_step_end(self, args, state, control, **kwargs):
        if self.pbar:
            self.pbar.update(1)

    def on_train_end(self, args, state, control, **kwargs):
        if self.pbar:
            self.pbar.close()
    
# Trainer ê°ì²´ ìƒì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    callbacks=[TqdmProgressCallback()]
)


# í˜„ì¬ í”„ë¡œì„¸ìŠ¤ê°€ ì‚¬ìš© ì¤‘ì¸ GPU ì •ë³´ ì¶œë ¥
print(f"ğŸ–¥ï¸  Rank {torch.cuda.current_device()} | GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}")

trainer.train()

# 11. ë§ˆì§€ë§‰ ëª¨ë¸ ì €ì¥
timestamp = time.strftime("%m%d-%H%M")
save_path = f"/home/selab/Desktop/Lightweight-main/output/checkpoint-last-{timestamp}"
model.save_pretrained(save_path, safe_serialization=True)
tokenizer.save_pretrained(save_path)

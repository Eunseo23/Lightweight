import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import torch
import re
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments
from datasets import Dataset
import pandas as pd
import math
import json
from transformers import TrainerCallback, TrainerControl, TrainerState, TrainingArguments
import time

# 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸° (ë¡œì»¬ ì €ì¥ ë¶ˆí•„ìš”)
model_name = "Salesforce/codet5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# 2. ì¶”ê°€í•  special tokens ì •ì˜
special_tokens = {
    "additional_special_tokens": ["<bug1>", "<bug2>", "<context>", "</context>", "<bug3>", "<bug4>", "<bug5>", "<bug6>", "<bug7>","<bug8>","<bug9>","<bug10>",
                                  "<bug11>","<bug12>","<bug13>","<bug14>","<bug15>","<bug16>","<bug17>","<bug18>","<bug19>","<bug20>", "</bug>", "<bug>",
                                  "<bug21>","<bug22>","<bug23>","<bug24>","<bug25>","<bug26>","<bug27>","<bug28>","<bug29>","<bug30>","<bug31>","<bug32>"]
}

# 3. í† í¬ë‚˜ì´ì €ì— special tokens ì¶”ê°€
num_added_tokens = tokenizer.add_special_tokens(special_tokens)

# 4. ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ ì¬ì¡°ì •
model.resize_token_embeddings(len(tokenizer))
embedding_weights = model.get_input_embeddings().weight
print("Embedding shape:", embedding_weights.shape)

def load_all_pf_pairs_from_dict(data, parent_key=""):
    pf_pairs = []

    if isinstance(data, list):
        top_level_items = enumerate(data)
    elif isinstance(data, dict):
        top_level_items = data.items()
    else:
        return pf_pairs

    for key, entry in top_level_items:
        if not isinstance(entry, dict):
            continue

        p_dict = {}
        f_dict = {}

        for file_key, value in entry.items():
            if file_key.endswith("_P"):
                base = file_key[:-2]
                p_dict[base] = value
            elif file_key.endswith("_F"):
                base = file_key[:-2]
                f_dict[base] = value

        for base in set(p_dict.keys()) & set(f_dict.keys()):
            pf_pairs.append({
                "source_file": str(parent_key),
                "group": str(key),
                "base": base,
                "buggy_code": p_dict[base],
                "fixed_code": f_dict[base]
            })

    return pf_pairs

def collect_all_pf_pairs_from_directory(directory_path):
    all_pairs = []

    for filename in os.listdir(directory_path):
        if filename.endswith(".json"):
            file_path = os.path.join(directory_path, filename)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                pairs = load_all_pf_pairs_from_dict(data, parent_key=filename)
                all_pairs.extend(pairs)
            except Exception as e:
                print(f"âŒ íŒŒì¼ '{filename}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return all_pairs

# ì •í™•í•œ íŒŒì¼ ê²½ë¡œ (MyDrive ê¸°ì¤€)
json_path = "to/your/path"
pf_pairs = collect_all_pf_pairs_from_directory(json_path)

# 5. buggy/fixed ì½”ë“œ ì¶”ì¶œ ë° í•„í„°ë§
processed_data = []
before_count = 0
after_count = 0

for pair in pf_pairs:
    buggy_code = pair["buggy_code"]
    fixed_code = pair["fixed_code"]

    before_count += 1

    # <bug1> ê°™ì€ íŠ¹ì • íƒœê·¸ ì¡°ê±´ í•„í„°ë§ (í•„ìš” ì‹œ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ)
    if isinstance(fixed_code, str) and re.match(r'^\s*<bug1>', fixed_code):
        processed_data.append({
            "input": buggy_code,
            "target": fixed_code
        })
        after_count += 1

    # # ê·¸ëƒ¥ ì¼ë°˜ ìŒ ì´ì½”ë“œ ì‚¬ìš©
    # processed_data.append({
    #     "input": buggy_code,
    #     "target": fixed_code
    # })

#ë°ì´í„° ìŒ ê°œìˆ˜ í™•ì¸
print(f"Before filtering: {before_count} bug-fix pairs")
print(f"After filtering: {after_count} bug-fix pairs")

# 6. Pandas DataFrameìœ¼ë¡œ ë³€í™˜
df = pd.DataFrame(processed_data)

# 7. Hugging Face Datasetìœ¼ë¡œ ë³€í™˜
dataset = Dataset.from_pandas(df)

# 8. ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_function(examples):
    prefix = "fix: "
    inputs = [prefix + ex for ex in examples["input"]]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["target"], max_length=512, truncation=True, padding="max_length")["input_ids"]

    labels = [[l if l != tokenizer.pad_token_id else -100 for l in label] for label in labels]
    model_inputs["labels"] = labels
    return model_inputs

# 6. ì „ì²˜ë¦¬ + ë¶„í• 
split_dataset = dataset.train_test_split(test_size=0.2)
tokenized_dataset = split_dataset.map(preprocess_function, batched=True)
train_dataset = tokenized_dataset["train"]
eval_dataset = tokenized_dataset["test"]


# ğŸ”§ ì—¬ê¸°ë§Œ ë°”ê¿”ì£¼ì„¸ìš”
dataset_size = len(train_dataset)  # ì˜ˆ: 7000ê°œë“  50ë§Œ ê°œë“  ìë™ ê³„ì‚°ë¨
batch_size = 16
accum_steps = 2
# epochs = 3

training_args = Seq2SeqTrainingArguments(
    output_dir="./Lightweight-main/python/output",
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    gradient_accumulation_steps=accum_steps,
    # âœ… CodeT5 ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš©
    learning_rate=3e-4,
    weight_decay=0.01,
    adam_epsilon=1e-8,
    max_grad_norm=1.0,

    warmup_steps=int(10000 * 0.06),  # ì§ì ‘ ì§€ì • or ê³„ì‚°
    max_steps=10000,  # âœ… ê³ ì • step ìˆ˜ 20000/10000
    evaluation_strategy="steps",
    eval_steps=1000,  # âœ… 2000/1000 stepë§ˆë‹¤ í‰ê°€
    save_strategy="steps",
    save_steps=1000,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    save_total_limit=1,

    logging_steps=500,
    logging_dir="./Lightweight-main/python/output/logs",
    fp16=True,
    predict_with_generate=False,
    disable_tqdm=False,
    report_to="none"
)

# 10. Trainer ì •ì˜ ë° í•™ìŠµ ì‹¤í–‰
class QuietEvalTrainer(Seq2SeqTrainer):
    def evaluation_loop(self, dataloader=None, description="Evaluation", prediction_loss_only=None, ignore_keys=None, metric_key_prefix="eval"):
        # ğŸ‘‡ í‰ê°€ ì¤‘ì—ëŠ” tqdm ë¹„í™œì„±í™”
        original_tqdm_state = self.args.disable_tqdm
        self.args.disable_tqdm = True

        result = super().evaluation_loop(
            dataloader=dataloader,
            description=description,
            prediction_loss_only=prediction_loss_only,
            ignore_keys=ignore_keys,
            metric_key_prefix=metric_key_prefix
        )

        # ğŸ‘‡ í‰ê°€ ëë‚˜ë©´ ì›ë˜ëŒ€ë¡œ ë³µì›
        self.args.disable_tqdm = original_tqdm_state
        return result
    
class ProgressPrinterCallback(TrainerCallback):
    def __init__(self, total_steps):
        self.start_time = None
        self.total_steps = total_steps

    def on_train_begin(self, args, state, control, **kwargs):
        self.start_time = time.time()
        print(f"ğŸš€ Training started. Total steps: {self.total_steps}")

    def on_log(self, args, state, control, logs=None, **kwargs):
        if self.start_time is None:
            return
        current_step = state.global_step
        elapsed = time.time() - self.start_time
        progress = current_step / self.total_steps
        eta = elapsed / progress - elapsed if progress > 0 else 0
        print(f"ğŸ”„ Step {current_step}/{self.total_steps} "
              f"({progress*100:.1f}%) | "
              f"Elapsed: {elapsed/60:.1f}m | ETA: {eta/60:.1f}m")

# âœ… ê·¸ ì•„ë˜ trainer ì •ì˜ ì‹œì—ë§Œ ë°”ê¿”ì£¼ë©´ ë!
trainer = QuietEvalTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    callbacks=[ProgressPrinterCallback(total_steps=training_args.max_steps)]
)

# í˜„ì¬ í”„ë¡œì„¸ìŠ¤ê°€ ì‚¬ìš© ì¤‘ì¸ GPU ì •ë³´ ì¶œë ¥
print(f"ğŸ–¥ï¸  Rank {torch.cuda.current_device()} | GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}")

trainer.train()

# 11. ë§ˆì§€ë§‰ ëª¨ë¸ ì €ì¥
timestamp = time.strftime("%m%d-%H%M")
save_path = f"./Lightweight-main/python/output/checkpoint-last-{timestamp}"
trainer.save_model(save_path)
tokenizer.save_pretrained(save_path)

with open(f"./Lightweight-main/python/output/training_log_{timestamp}.json", "w") as f:
    json.dump(trainer.state.log_history, f, indent=2)
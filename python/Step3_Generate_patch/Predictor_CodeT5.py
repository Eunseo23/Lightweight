from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import os
import json
import re

# 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©
model_path = "./Lightweight-main/python/output/checkpoint-last-0421-2157"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

# 2. ì¶”ê°€í•  special tokens ì •ì˜
special_tokens = {
    "additional_special_tokens": ["<bug1>", "<bug2>", "<context>", "</context>", "<bug3>", "<bug4>", "<bug5>", "<bug6>", "<bug7>","<bug8>","<bug9>","<bug10>",
                                  "<bug11>","<bug12>","<bug13>","<bug14>","<bug15>","<bug16>","<bug17>","<bug18>","<bug19>","<bug20>", "</bug>", "<bug>"]
}

# 3. í† í¬ë‚˜ì´ì €ì— special tokens ì¶”ê°€
num_added_tokens = tokenizer.add_special_tokens(special_tokens)

# 4. ëª¨ë¸ ì„ë² ë”© ì‚¬ì´ì¦ˆ ì¬ì¡°ì •
model.resize_token_embeddings(len(tokenizer))


device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
model.eval()

# 2. ì˜ˆì¸¡ í•¨ìˆ˜ (unique 500ê°œ ì¶”ì¶œí•˜ê¸°)
# def predict_unique_variants(input_code: str, desired_unique: int = 500, batch_size: int = 100, max_length: int = 512):
#     prefix = "fix: "
#     input_text = prefix + input_code
#     inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding=True, max_length=max_length).to(device)

#     unique_outputs = set()
#     tries = 0

#     while len(unique_outputs) < desired_unique and tries < 20:  # ìµœëŒ€ 20ë²ˆ ë°˜ë³µ
#         outputs = model.generate(
#             **inputs,
#             do_sample=True,
#             top_k=50,
#             top_p=0.95,
#             temperature=0.9,
#             repetition_penalty=1.2,
#             num_return_sequences=batch_size,
#             max_length=max_length,
#         )
#         #ëª¨ë¸ ì¶œë ¥ ë””ì½”ë”© skip_special_tokens=Falseë¡œ í•´ì•¼ <bug1>íƒœê·¸ ë¶™ì–´ì„œ ê²°ê³¼ ì¶œë ¥
#         decoded = tokenizer.batch_decode(outputs, skip_special_tokens=False)
#         # ë³´ê³  ì‹¶ì€ í† í°ë§Œ ë‚¨ê¸°ê¸°
#         allowed_tags = {f"<bug{i}>" for i in range(1, 33)} | {"<bug>", "</bug>"} 
#         cleaned = []

#         for line in decoded:
#             tokens = line.split()
#             filtered = [tok for tok in tokens if tok in allowed_tags or not tok.startswith("<")]
#             cleaned.append(" ".join(filtered))


#         unique_outputs.update([o.strip() for o in cleaned])
#         tries += 1
#         print(f"ğŸ” {tries}íšŒì°¨ ìƒì„± | ê³ ìœ  ê²°ê³¼ ìˆ˜: {len(unique_outputs)}")

#     return list(unique_outputs)[:desired_unique]  # ë”± 500ê°œë§Œ ì˜ë¼ì„œ ë°˜í™˜


##########################################################################################
def predict_unique_variants(input_code: str, batch_size: int = 100, max_length: int = 512):
    prefix = "fix: "
    input_text = prefix + input_code
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding=True, max_length=max_length).to(device)

    all_cleaned = []

    for i in range(1):  # ğŸ” 5ë²ˆë§Œ ë°˜ë³µ
        outputs = model.generate(
            **inputs,
            do_sample=True,
            top_k=100,
            top_p=0.95,
            temperature=1.0,
            repetition_penalty=1.2,
            num_return_sequences=batch_size,
            max_length=max_length,
        )
        
        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=False) #Trueí† í° ì œê±°, False í† í° ì¶”ê°€ 
        filter_tags = True  # âœ… ì—¬ê¸°ì„œ <bugX> íƒœê·¸ í¬í•¨í• ì§€ ê²°ì • Falseí† í°ì œê±°, Trueí† í° í¬í•¨
        allowed_tags = {f"<bug{i}>" for i in range(1, 33)} 
        unwanted_tokens = {"<pad>", "<s>", "</s>", "<unk>"}
        cleaned = []

        for line in decoded:
            # ğŸ”§ íŠ¹ìˆ˜ í† í° ë¨¼ì € ì œê±°
            for tok in unwanted_tokens:
                line = line.replace(tok, "")

            tokens = line.split()

            if filter_tags:
                # í—ˆìš©ëœ íƒœê·¸ or ì¼ë°˜ í† í°ë§Œ ë‚¨ê¸°ê¸°
                filtered = [tok for tok in tokens if tok in allowed_tags or not tok.startswith("<")]
            else:
                filtered = tokens

            cleaned.append(" ".join(filtered))

        all_cleaned.extend(cleaned)
        print(f"ğŸ” {i+1}íšŒì°¨ ìƒì„± ì™„ë£Œ | ëˆ„ì  ì´ í›„ë³´: {len(all_cleaned)}")

    # âœ… ì¤‘ë³µ ì œê±°
    unique_outputs = list(dict.fromkeys([o.strip() for o in all_cleaned]))
    print(f"âœ… ê³ ìœ í•œ ê²°ê³¼ ê°œìˆ˜: {len(unique_outputs)}")

    return unique_outputs

# 4. ì…ë ¥ íŒŒì¼ ê²½ë¡œ
# base_dir = "to/your/path"
# input_path = os.path.join(base_dir, "Lightweight_buggy_method_Context.txt")
# output_path = os.path.join(base_dir, "lWCP.txt")

# with open(input_path, "r", encoding="utf-8") as f:
#     buggy_code = f.read()

# # 5. ì‹¤í–‰
# predictions = predict_unique_variants(buggy_code)

# # 6. ê²°ê³¼ ì €ì¥
# with open(output_path, "w", encoding="utf-8") as f:
#     for pred in predictions:
#         f.write(pred + "\n")

# print(f"âœ… ì´ {len(predictions)}ê°œì˜ ì¤‘ë³µ ì œê±°ëœ ì¶œë ¥ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def process_json_file(json_path):
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    folder_key = next(iter(data.keys()))
    items = data[folder_key]

    global_lwcp_index = 1  # ì „ì—­ lwcp ë²ˆí˜¸ ì‹œì‘

    for item in items:
        lwbm = item["lwbm"]
        predictions = predict_unique_variants(lwbm)

        # ì „ì—­ ë²ˆí˜¸ë¡œ lwcp ì €ì¥
        item["lwcp"] = {}
        for pred in sorted(predictions):
            item["lwcp"][f"lwcp{global_lwcp_index}"] = pred
            global_lwcp_index += 1

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

    print(f"âœ… {json_path} ì²˜ë¦¬ ì™„ë£Œ. {len(items)}ê°œì˜ í•­ëª©ì— ì´ {global_lwcp_index - 1}ê°œì˜ lwcp ì €ì¥ë¨.")

def main():
    base_dir = "to/your/path"

    # Chart_* í´ë” ìˆœíšŒ
    for folder in os.listdir(base_dir):
        folder_path = os.path.join(base_dir, folder)
        if os.path.isdir(folder_path):  # Chart ì—¬ë¶€ ì œí•œ ì œê±°
            # ë‚´ë¶€ JSON íŒŒì¼ ì°¾ê¸°
            for file_name in os.listdir(folder_path):
                if re.match(r"Lightweight_buggy_method_Context.*\.json$", file_name):
                    json_path = os.path.join(folder_path, file_name)
                    try:
                        process_json_file(json_path)
                    except Exception as e:
                        print(f"âŒ {json_path} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()
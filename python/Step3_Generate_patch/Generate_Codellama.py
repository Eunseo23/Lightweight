import os
import re
import fnmatch 
import json
import torch
import torch.nn as nn
from itertools import chain
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, BitsAndBytesConfig
from peft import PeftModel


# ====== ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸° ======
def load_codellama_lora_model(model_path):
    try:
        # âœ… tokenizer ì„¤ì •
        tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLLaMA-7b-hf", use_fast=False)
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.add_special_tokens({"additional_special_tokens": ["[bug_function], [reference_function], [fix_code]"]})
        quant_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
            llm_int8_skip_modules=None,  # í•„ìš” ì‹œ ì„¤ì •
        )

        base_model = AutoModelForCausalLM.from_pretrained(
            "codellama/CodeLLaMA-7b-hf",
            torch_dtype=torch.float16,
            device_map="auto",
            quantization_config=quant_config,
            trust_remote_code=True
        )
        base_model.resize_token_embeddings(len(tokenizer))

        # âœ… LoRA ì–´ëŒ‘í„° ì ìš©
        model = PeftModel.from_pretrained(base_model, model_path, torch_dtype=torch.float16)
        # model.resize_token_embeddings(len(tokenizer))
        model.eval()

        # âœ… ë””ë°”ì´ìŠ¤ ì •ë³´ ì¶œë ¥
        device = next(model.parameters()).device
        print(f"âœ… ëª¨ë¸ ì£¼ìš” íŒŒë¼ë¯¸í„°ê°€ ì˜¬ë¼ê°„ ë””ë°”ì´ìŠ¤: {device}")
        if torch.cuda.is_available():
            print(f"ğŸ¯ GPU ì´ë¦„: {torch.cuda.get_device_name(device.index)}")

        return model, tokenizer

    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None

# ====== ì˜ˆì¸¡ í•¨ìˆ˜ ======
# def trim_after_bug(pred: str, lwbm: str) -> str:
#     """
#     ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ <bug>...</bug> ì´í›„ <context> ì´ì „ê¹Œì§€ë§Œ ë‚¨ê¹€.
#     """
#     # </bug> ì™€ <context> ì‚¬ì´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
#     post_bug_match = re.search(r"</bug>(.*?)<context>", lwbm, re.DOTALL)
#     if not post_bug_match:
#         return pred.strip()

#     post_bug_code = post_bug_match.group(1).strip()
    
#     if not post_bug_code:
#         # ì‚¬ì´ì— ì•„ë¬´ ì½”ë“œ ì—†ìœ¼ë©´ ì˜ë¼ë‚¼ ê²Œ ì—†ìŒ
#         return pred.strip()

#     if post_bug_code in pred:
#         pred = pred.split(post_bug_code, 1)[0] + post_bug_code

#     return pred.strip()

def generate_single_prediction(pair, model, tokenizer, device, max_new_tokens=512, total=10, chunk_size=10, diversity_penalty=1.3):
    # prefix_prompt = ""
    buggy_code = pair["lwbm"]
    
    bug_end_marker = "\n// bug_end\n"
    fix_end_marker = "\n// fix_end\n"
    bug_end_count = buggy_code.count(bug_end_marker)
    
    input_text = "\n[bug_function]\n" + buggy_code + "\n[fix_code]\n"# âœ… prefix í¬í•¨í•œ ì…ë ¥
    all_predictions = set()

    inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=1024)
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)
    inputs_len = input_ids.shape[1] 

    num_iterations = total // chunk_size

    with torch.no_grad():
        for _ in range(num_iterations):
            outputs = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                # â†“ ì—¬ê¸°ë¶€í„° ì¶”ê°€ëœ ì˜µì…˜
                num_beams=chunk_size,
                num_beam_groups=chunk_size,
                diversity_penalty=diversity_penalty,
                num_return_sequences=chunk_size,
                # â†‘ ì¶”ê°€ ë
                # num_beams=chunk_size,                # ğŸ” ë¹”ì˜ ê°œìˆ˜ ì„¤ì • (ì˜ˆ: 5ê°œ ë¹”)
                # num_return_sequences=chunk_size,     # ğŸ” ìµœì¢…ìœ¼ë¡œ ë°˜í™˜í•  ì‹œí€€ìŠ¤ ê°œìˆ˜ (ex. top 10)
                early_stopping=True,         # âœ”ï¸ ë¹” ëª¨ë‘ EOS ë§Œë‚˜ë©´ ë©ˆì¶¤
                # early_stopping=False,
                max_new_tokens=max_new_tokens,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                do_sample=False              # ğŸ” Beam searchëŠ” sampling êº¼ì•¼ ì•ˆì •ì 
                )
            
            # outputs = model.generate(
            #     input_ids=input_ids,
            #     attention_mask=attention_mask,
            #     do_sample=True,
            #     top_k=50,
            #     top_p=0.9,
            #     temperature=0.5,
            #     max_new_tokens=max_new_tokens,
            #     num_return_sequences=chunk_size,
            #     pad_token_id=tokenizer.pad_token_id,
            #     eos_token_id=tokenizer.eos_token_id
            # )
            
            output_ids = outputs[:, inputs_len:]
            decoded_outputs = tokenizer.batch_decode(
                output_ids,
                # outputs,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )

            # for pred in decoded_outputs:
            #     # clean_pred = trim_after_bug(pred, buggy_code)
            #     all_predictions.add(pred.strip())

            for pred in decoded_outputs:
                trimmed = pred.strip()
                fix_count = trimmed.count(fix_end_marker)
                
                if fix_count > bug_end_count > 0:
                    parts = trimmed.split(fix_end_marker)
                    allowed = parts[:bug_end_count]
                    trimmed = fix_end_marker.join(allowed) + fix_end_marker
                elif fix_count > 0 and bug_end_count == 0 :
                    trimmed = trimmed.split(fix_end_marker)[0]
                
                all_predictions.add(trimmed)
            
            torch.cuda.empty_cache()

    return sorted(all_predictions)[:total]

# ====== JSON ì²˜ë¦¬ ë° ì˜ˆì¸¡ ì €ì¥ ======
def process_json_file(json_path, model, tokenizer):
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        folder_key = next(iter(data.keys()))
        items = data[folder_key]
        lwcp_counter = 1  # âœ… ì „ì²´ JSON ê¸°ì¤€ìœ¼ë¡œ ìˆœì°¨ ì¦ê°€í•  ë²ˆí˜¸
        
        # items = [i for i in items if i.get("id") == "1_50%"]
        for item in tqdm(items, desc=f"Generating lwcp for {folder_key}"):
            pred_list = generate_single_prediction(item, model, tokenizer, device)
            unique_predictions = sorted(set(pred_list))[:100]

            item["lwcp"] = {}

            if unique_predictions:
                for p in unique_predictions:
                    item["lwcp"][f"lwcp{lwcp_counter}"] = p
                    lwcp_counter += 1
            else:
                # ì˜ˆì¸¡ì´ ì•„ë¬´ê²ƒë„ ì—†ì„ ê²½ìš°ì—ë„ lwcp1 ë„£ê¸°
                item["lwcp"][f"lwcp{lwcp_counter}"] = "empty"
                lwcp_counter += 1

        # âœ… ìƒˆ íŒŒì¼ëª… ìƒì„±
        new_filename = os.path.basename(json_path).replace(".json", "_codellama.json")
        new_path = os.path.join(os.path.dirname(json_path), new_filename)

        with open(new_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        print(f"âœ… ì €ì¥ ì™„ë£Œ: {new_path}")
        return True

    except Exception as e:
        print(f"âŒ {json_path} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

# def process_all_txt_in_base_dir(base_dir, model, tokenizer):
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     failed_files = []

#     for project_folder in sorted(os.listdir(base_dir)):
#         # âœ… Lightweight í´ë”ë§Œ ì²˜ë¦¬í•˜ê³  ì‹¶ë‹¤ë©´:
#         if project_folder != "Token300":
#             print(f"ğŸš« {project_folder} í´ë”ëŠ” ìŠ¤í‚µë¨.")
#             continue

#         project_path = os.path.join(base_dir, project_folder)
#         if not os.path.isdir(project_path):
#             continue

#         for case_folder in sorted(os.listdir(project_path)):
#             case_path = os.path.join(project_path, case_folder)
#             if not os.path.isdir(case_path):
#                 continue

#             txt_candidates = [f for f in os.listdir(case_path)
#                               if ('Original_buggy_method' in f and 'Original_buggy_method_by_line' not in f and f.endswith('.txt'))]

#             if not txt_candidates:
#                 print(f"âš ï¸ {case_path} ë‚´ ì²˜ë¦¬í•  TXT ì—†ìŒ.")
#                 continue

#             for txt_file in txt_candidates:
#                 txt_path = os.path.join(case_path, txt_file)
#                 try:
#                     generate_patch_for_txt_file(txt_path, model, tokenizer, device)
#                 except Exception as e:
#                     print(f"âŒ ì‹¤íŒ¨: {txt_path} | ì—ëŸ¬: {e}")
#                     failed_files.append(txt_path)

#     if failed_files:
#         print("\nâ— ì²˜ë¦¬ ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡:")
#         for f in failed_files:
#             print("  -", f)
#     else:
#         print("\nğŸ‰ ëª¨ë“  TXT íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")


# # ====== ì„¤ì • ======
# model_path = "./Lightweight-main/output/checkpoint-last-0316-1124/pytorch_model_0316-1124.bin"
# base_dir = "to/your/path"

# if __name__ == "__main__":
#     model, tokenizer = load_model_and_tokenizer()
#     process_all_txt_in_base_dir(base_dir, model, tokenizer)


#================== Original_buggy_method.txt íŒŒì¼ 500ê°œ ìƒì„±==================
# def generate_patch_for_txt_file(txt_path, model, tokenizer, device):
#     """txt íŒŒì¼ì˜ ì²« ì¤„ì„ ì½ê³  ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ 100ê°œë¥¼ candidate_patch*.txtì— ì €ì¥"""
#     with open(txt_path, 'r', encoding='utf-8') as f:
#         first_line = f.readline().strip()

#     if not first_line:
#         print(f"âš ï¸ {txt_path} ì²« ì¤„ì´ ë¹„ì–´ ìˆìŒ. ê±´ë„ˆëœ€.")
#         return

#     pred_list = generate_single_prediction(first_line, model, tokenizer, device)
#     unique_predictions = sorted(set(pred_list))[:500]

#     # ì¶œë ¥ íŒŒì¼ëª… êµ¬ì„±
#     filename = os.path.basename(txt_path)
#     patch_filename = filename.replace("Original_buggy_method", "candidate_patch_codellama")
#     patch_path = os.path.join(os.path.dirname(txt_path), patch_filename)

#     with open(patch_path, 'w', encoding='utf-8') as f:
#         for pred in unique_predictions:
#             f.write(pred + '\n')

#     print(f"âœ… {patch_path} ì €ì¥ ì™„ë£Œ ({len(unique_predictions)}ê°œ íŒ¨ì¹˜)")



# # ====== ì „ì²´ í´ë” ë‚´ JSON ìˆœíšŒ ì²˜ë¦¬ ======
# def process_all_json_in_base_dir(base_dir, model, tokenizer):
#     failed_folders = []

#     for mid_folder in sorted(os.listdir(base_dir)):
#         mid_folder_path = os.path.join(base_dir, mid_folder)
#         if not os.path.isdir(mid_folder_path):
#             continue

#         for chart_folder in sorted(os.listdir(mid_folder_path)):
#             chart_folder_path = os.path.join(mid_folder_path, chart_folder)
#             if not os.path.isdir(chart_folder_path):
#                 continue

#             json_files = [f for f in os.listdir(chart_folder_path) if f.endswith(".json")]
#             if not json_files:
#                 print(f"âš ï¸ {chart_folder_path} ë‚´ JSON íŒŒì¼ ì—†ìŒ. ê±´ë„ˆëœ€.")
#                 continue

#             for json_file in json_files:
#                 json_path = os.path.join(chart_folder_path, json_file)
#                 success = process_json_file(json_path, model, tokenizer)
#                 if not success:
#                     failed_folders.append(chart_folder_path)

#     if failed_folders:
#         print("\nâ— ì²˜ë¦¬ ì‹¤íŒ¨í•œ í´ë” ëª©ë¡:")
#         for folder in failed_folders:
#             print("  -", folder)
#     else:
#         print("\nğŸ‰ ëª¨ë“  JSON íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")

# ====== ì „ì²´ í´ë” ë‚´ íŠ¹ì • JSONíŒŒì¼ë§Œ ìˆœíšŒ ì²˜ë¦¬ ======
def process_all_json_in_base_dir(base_dir, model, tokenizer):
    failed_folders = []

    for project_folder in sorted(os.listdir(base_dir)):
        if project_folder != "Diffllama":  # âœ… Lightweight í´ë”ë§Œ ì²˜ë¦¬
        # if project_folder in ["Codellama", "multi"]:
        # if project_folder = "Lightweight":  # âœ… Lightweight í´ë”ë§Œ ì œì™¸
            print(f"ğŸš« {project_folder} í´ë”ëŠ” ìŠ¤í‚µë¨.")
            continue

        project_path = os.path.join(base_dir, project_folder)
        if not os.path.isdir(project_path):
            continue

        for case_folder in sorted(os.listdir(project_path)):
            case_path = os.path.join(project_path, case_folder)
            if not os.path.isdir(case_path):
                continue

            json_candidates = [f for f in os.listdir(case_path)
                            if fnmatch.fnmatch(f, "*Lightweight_method_llama_diff*.json")
                            and "codellama" not in f]

            # json_candidates = [f for f in os.listdir(case_path)
            #     if f == "Lightweight_buggy_method_Context.json"]

            if not json_candidates:
                print(f"âš ï¸ {case_path} ë‚´ ëŒ€ìƒ JSON ì—†ìŒ. ê±´ë„ˆëœ€.")
                continue

            for json_filename in json_candidates:
                json_path = os.path.join(case_path, json_filename)
                success = process_json_file(json_path, model, tokenizer)
                if not success:
                    failed_folders.append(case_path)

    if failed_folders:
        print("\nâ— ì²˜ë¦¬ ì‹¤íŒ¨í•œ í´ë” ëª©ë¡:")
        for folder in failed_folders:
            print("  -", folder)
    else:
        print("\nğŸ‰ ëª¨ë“  JSON íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")


# ====== ì„¤ì • ======
model_path = "./Lightweight-main/output/checkpoint-last-0723-2121"
base_dir = "to/your/path"

if __name__ == "__main__":
    model, tokenizer = load_codellama_lora_model(model_path)
    process_all_json_in_base_dir(base_dir, model, tokenizer)


# ====== ë‹¨ì¼ í´ë” ë‚´ JSON ìˆœíšŒ ì²˜ë¦¬ ======
# def process_all_json_in_target_folder(target_folder, model, tokenizer):
#     failed_files = []

#     if not os.path.isdir(target_folder):
#         print(f"â— ìœ íš¨í•˜ì§€ ì•Šì€ í´ë” ê²½ë¡œì…ë‹ˆë‹¤: {target_folder}")
#         return

#     json_files = [f for f in os.listdir(target_folder) if f.endswith(".json")]
#     if not json_files:
#         print(f"âš ï¸ {target_folder} ë‚´ JSON íŒŒì¼ ì—†ìŒ. ê±´ë„ˆëœ€.")
#         return

#     for json_file in json_files:
#         json_path = os.path.join(target_folder, json_file)
#         success = process_json_file(json_path, model, tokenizer)
#         if not success:
#             failed_files.append(json_file)

#     if failed_files:
#         print("\nâ— ì²˜ë¦¬ ì‹¤íŒ¨í•œ JSON íŒŒì¼ ëª©ë¡:")
#         for file in failed_files:
#             print("  -", file)
#     else:
#         print(f"\nğŸ‰ ëª¨ë“  JSON íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ! (ì´ {len(json_files)}ê°œ)")

# # ====== ì„¤ì • ======
# model_path = "./Lightweight-main/python/output/checkpoint-last-0316-1124/pytorch_model_0316-1124.bin"
# target_folder = "to/your/path"  # ğŸ” ì—¬ê¸°ë¥¼ ì›í•˜ëŠ” í´ë”ë¡œ ìˆ˜ì •

# if __name__ == "__main__":
#     model, tokenizer = load_model_and_tokenizer()
#     process_all_json_in_target_folder(target_folder, model, tokenizer)


# ====== ë‹¨ì¼ í´ë” ë‚´ íŠ¹ì •JSONíŒŒì¼ë§Œ ìˆœíšŒ ì²˜ë¦¬ ======
# def process_specific_json_in_target_folder(target_folder, model, tokenizer, target_keyword):
#     failed_files = []

#     if not os.path.isdir(target_folder):
#         print(f"â— ìœ íš¨í•˜ì§€ ì•Šì€ í´ë” ê²½ë¡œì…ë‹ˆë‹¤: {target_folder}")
#         return

#     # í´ë” ë‚´ì—ì„œ target_keywordë¥¼ í¬í•¨í•˜ê³  .json í™•ì¥ìì¸ ëª¨ë“  íŒŒì¼ì„ ì°¾ìŒ
#     json_files = [
#         f for f in os.listdir(target_folder)
#         if f.endswith(".json") and target_keyword in f
#     ]

#     if not json_files:
#         print(f"âš ï¸ {target_folder} ë‚´ '{target_keyword}' í¬í•¨ëœ JSON ì—†ìŒ. ê±´ë„ˆëœ€.")
#         return

#     for json_file in json_files:
#         json_path = os.path.join(target_folder, json_file)
#         success = process_json_file(json_path, model, tokenizer)
#         if not success:
#             failed_files.append(json_file)

#     if failed_files:
#         print("\nâ— ì²˜ë¦¬ ì‹¤íŒ¨í•œ JSON íŒŒì¼ ëª©ë¡:")
#         for file in failed_files:
#             print("  -", file)
#     else:
#         print(f"\nğŸ‰ '{target_keyword}' í¬í•¨ëœ ëª¨ë“  JSON ì²˜ë¦¬ ì™„ë£Œ!")

# def process_multiple_folders(base_path, folder_list, model, tokenizer, target_keyword):
#     for folder_name in folder_list:
#         target_folder = os.path.join(base_path, folder_name)
#         process_specific_json_in_target_folder(target_folder, model, tokenizer, target_keyword)

# # ====== ì„¤ì • ======
# model_path = "./Lightweight-main/python/output/checkpoint-last-0316-1124/pytorch_model_0316-1124.bin"
# base_path = "./Lightweight-main/result_defects4j/version1/Lightweight"  # ì—¬ëŸ¬ í´ë”ë“¤ì´ ìˆëŠ” ê¸°ë³¸ ê²½ë¡œ
# # target_folders = ["Closure_65", "Chart_4", "Chart_26","Closure_66","Closure_119", "Chart_13", "Closure_71", "Closure_123","Lang_16","Lang_58", "Math_24"]  # ğŸ”¥ ì²˜ë¦¬í•˜ê³  ì‹¶ì€ í´ë”ë“¤ ë¦¬ìŠ¤íŠ¸
# target_folders = ["Math_38","Closure_65"] 
# target_keyword = "Lightweight_buggy_method_Context_diff"

# if __name__ == "__main__":
#     model, tokenizer = load_model_and_tokenizer()
#     # process_multiple_folders(base_path, target_folders, model, tokenizer, target_filename)
#     process_multiple_folders(base_path, target_folders, model, tokenizer, target_keyword)
#     # process_specific_json_in_target_folder(target_folders, model, tokenizer, target_filename)